{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c0538-9e4d-4abc-9a40-6dfc813fbe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0-data-exploration-and-preprocessing.ipynb\n",
    "This notebook is all about understanding your data, getting it squeaky clean, and preparing it for the machine learning model. Think of it as laying a solid foundation before building your house.\n",
    "\n",
    "Notebook Sections\n",
    "\n",
    "Introduction & Setup\n",
    "\n",
    "Briefly state the notebook's purpose: explore the dataset, preprocess images, and prepare data for training.\n",
    "Import necessary libraries: pandas, numpy, matplotlib.pyplot, seaborn, tensorflow, keras.preprocessing.image, sklearn.model_selection, os.\n",
    "Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "\n",
    "# Set up paths\n",
    "RAW_DATA_DIR = 'data/raw/dataset' # Adjust if your dataset is in a different subfolder\n",
    "PROCESSED_DATA_DIR = 'data/processed'\n",
    "TRAIN_DIR = os.path.join(PROCESSED_DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(PROCESSED_DATA_DIR, 'validation')\n",
    "TEST_DIR = os.path.join(PROCESSED_DATA_DIR, 'test')\n",
    "Data Loading & Initial Inspection\n",
    "\n",
    "Load the raw data (images). Since TrashNet is usually organized in folders by class, you'll iterate through directories.\n",
    "Get a list of all image paths and their corresponding labels.\n",
    "Display sample images from each class to get a visual sense of the data.\n",
    "Python\n",
    "# Collect all image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(RAW_DATA_DIR))\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(RAW_DATA_DIR, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for img_name in os.listdir(class_path):\n",
    "            image_paths.append(os.path.join(class_path, img_name))\n",
    "            labels.append(class_name)\n",
    "\n",
    "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
    "print(f\"Total images found: {len(df)}\")\n",
    "print(\"Class distribution:\\n\", df['label'].value_counts())\n",
    "\n",
    "# Display a few sample images\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_name in enumerate(class_names[:6]): # Display 6 classes\n",
    "    sample_img_path = df[df['label'] == class_name].sample(1)['image_path'].iloc[0]\n",
    "    img = plt.imread(sample_img_path)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Exploratory Data Analysis (EDA)\n",
    "\n",
    "Class Distribution: Visualize the number of images per class using a bar plot. Check for class imbalance.\n",
    "Image Dimensions: Analyze the distribution of image heights and widths. This helps in deciding on a target size for resizing.\n",
    "Identify potential issues: Are there corrupted images? Very small/large images?\n",
    "Python\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='label', palette='viridis')\n",
    "plt.title('Distribution of Waste Categories')\n",
    "plt.xlabel('Waste Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Analyze image dimensions - might be slow for very large datasets\n",
    "# from PIL import Image\n",
    "# img_widths = []\n",
    "# img_heights = []\n",
    "# for img_path in df['image_path']:\n",
    "#     try:\n",
    "#         with Image.open(img_path) as img:\n",
    "#             width, height = img.size\n",
    "#             img_widths.append(width)\n",
    "#             img_heights.append(height)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error opening image {img_path}: {e}\")\n",
    "# print(f\"Average image width: {np.mean(img_widths):.0f}, height: {np.mean(img_heights):.0f}\")\n",
    "Data Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test sets. A common split is 70% train, 15% validation, 15% test.\n",
    "Crucially, stratify the split to ensure each subset has a similar class distribution as the original dataset.\n",
    "Python\n",
    "# Split the data into train (70%), validation (15%), and test (15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42) # 0.5 of 30% is 15%\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Verify distribution in splits (optional)\n",
    "print(\"\\nTrain class distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nValidation class distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nTest class distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "Data Preprocessing & Augmentation\n",
    "\n",
    "Create directories for processed data (data/processed/train, data/processed/validation, data/processed/test).\n",
    "Copy images to their respective new directories. This makes it easy for ImageDataGenerator to pick them up.\n",
    "Define ImageDataGenerator for training (with augmentation) and validation/testing (only rescaling).\n",
    "Rescaling: Normalize pixel values (e.g., to [0, 1]).\n",
    "Augmentation: Apply transformations like rotation, zoom, flips, shifts. This helps the model generalize better by seeing diverse versions of the same image.\n",
    "Python\n",
    "# Create target directories if they don't exist\n",
    "for directory in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(directory, class_name), exist_ok=True)\n",
    "\n",
    "# Function to copy images to their respective processed folders\n",
    "def copy_images_to_processed(dataframe, target_dir):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        src_path = row['image_path']\n",
    "        dest_path = os.path.join(target_dir, row['label'], os.path.basename(src_path))\n",
    "        copyfile(src_path, dest_path)\n",
    "\n",
    "print(\"Copying train images...\")\n",
    "copy_images_to_processed(train_df, TRAIN_DIR)\n",
    "print(\"Copying validation images...\")\n",
    "copy_images_to_processed(val_df, VAL_DIR)\n",
    "print(\"Copying test images...\")\n",
    "copy_images_to_processed(test_df, TEST_DIR)\n",
    "\n",
    "# Define image size for the model\n",
    "IMG_SIZE = (224, 224) # Common size for pre-trained CNNs\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,             # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,          # Random rotation\n",
    "    width_shift_range=0.2,      # Random horizontal shift\n",
    "    height_shift_range=0.2,     # Random vertical shift\n",
    "    shear_range=0.2,            # Shear transformations\n",
    "    zoom_range=0.2,             # Random zoom\n",
    "    horizontal_flip=True,       # Random horizontal flips\n",
    "    fill_mode='nearest'         # Strategy for filling in new pixels created by transformations\n",
    ")\n",
    "\n",
    "# Validation and test generators should only rescale\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False # No need to shuffle test data\n",
    ")\n",
    "\n",
    "print(\"\\nData generators created successfully!\")\n",
    "print(f\"Number of classes: {train_generator.num_classes}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")\n",
    "\n",
    "# Save class names mapping for later use (e.g., in predict.py)\n",
    "class_indices_df = pd.DataFrame(train_generator.class_indices.items(), columns=['class_name', 'index'])\n",
    "class_indices_df.to_csv(os.path.join(PROCESSED_DATA_DIR, 'class_indices.csv'), index=False)\n",
    "Conclusion\n",
    "\n",
    "Summarize the data preparation steps and the readiness of the data for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
